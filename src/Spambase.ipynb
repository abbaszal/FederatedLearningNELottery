{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5b3f5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import ast\n",
    "import copy\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Sequence, Union\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain, combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from utils.Spambase.split_data import split_data_equal\n",
    "from utils.aggregate_functions import aggregate_lr_models, FederatedForest\n",
    "from utils.evaluate_coalitions_new import evaluate_coalitions2\n",
    "from utils.DecisionTree import DecisionTree\n",
    "from utils.nash1 import find_nash_equilibria_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce757232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_fedlr(partitions, random_seed, X_test, y_test, max_iter):\n",
    "    client_models = []\n",
    "    client_global_accuracies = []\n",
    "    \n",
    "    for X_i, y_i in partitions:\n",
    "        nan_mask = ~np.isnan(X_i).any(axis=1)\n",
    "        X_clean = X_i[nan_mask]\n",
    "        y_clean = y_i[nan_mask]\n",
    "        if len(y_clean) == 0:\n",
    "            client_models.append(None)\n",
    "            client_global_accuracies.append(None)\n",
    "            continue\n",
    "        \n",
    "        model = LogisticRegression(random_state=random_seed, max_iter=max_iter)\n",
    "        try:\n",
    "            local_scaler = StandardScaler()\n",
    "            model.fit(local_scaler.fit_transform(X_clean), y_clean)\n",
    "            client_models.append(model)\n",
    "            client_global_accuracies.append(model.score(X_test, y_test))\n",
    "        except Exception as e:\n",
    "            client_models.append(None)\n",
    "            client_global_accuracies.append(None)\n",
    "    \n",
    "    return client_models, client_global_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e0ced08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_fedfor(partitions, X_test, y_test, max_depth, trial_seed):\n",
    "    client_models = []\n",
    "    client_global_accuracies = {}\n",
    "    \n",
    "    for i, (X_i, y_i) in enumerate(partitions):\n",
    "        model = DecisionTreeClassifier(\n",
    "            max_depth=max_depth,\n",
    "            random_state=trial_seed\n",
    "        )\n",
    "        model.fit(X_i, y_i)\n",
    "        client_models.append(model)\n",
    "\n",
    "        # Evaluate directly on the fixed global test set\n",
    "        y_pred = model.predict(X_test)\n",
    "        client_global_accuracies[i] = np.mean(y_pred == y_test)\n",
    "    \n",
    "    return client_models, client_global_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84459b66",
   "metadata": {},
   "source": [
    "### FedLR _ Spambase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "903aac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/abbaszal/Documents/Thesis_Project_Spambase/data/spambase.data'  # Adjust the path as needed\n",
    "df = pd.read_csv(file_path, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9180b9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "BUNDLE_PATH = '/.../tree_shape_no_level_none_delta_noN_leaveout_spambase.joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fbb4a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = df.iloc[:, :-1].to_numpy()\n",
    "y_full = df.iloc[:, -1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b530f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-8\n",
    "n_trials = 100\n",
    "n_clients_list = [10,20,30,40,50,60,70,80,90,100]\n",
    "base_seed = 42\n",
    "max_iters = [100]\n",
    "approach = 'fedlr'\n",
    "\n",
    "save_dir = \"/.../spambase_fedlr_tree\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31905d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_bundle = joblib.load(BUNDLE_PATH)\n",
    "_PIPE = _bundle[\"pipeline\"]\n",
    "_SAVED_FEATURES = _bundle[\"features\"]\n",
    "\n",
    "\n",
    "_META = {\n",
    "    \"selected_features\": list(_SAVED_FEATURES),\n",
    "    \"featureset\": \"shape_no_level\",\n",
    "    \"include_n_clients\": False,  \n",
    "    \"target_mode\": \"delta\",\n",
    "    \"logit_target\": False,\n",
    "}\n",
    "\n",
    "def _sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def _base_stats_from_accs(accs_1d):\n",
    "    v = np.asarray(accs_1d, dtype=float)\n",
    "    return {\n",
    "        'mean': float(np.mean(v)),\n",
    "        'median': float(np.median(v)),\n",
    "        'max': float(np.max(v)),\n",
    "        'percentile_90': float(np.percentile(v, 90)),\n",
    "        'percentile_75': float(np.percentile(v, 75)),\n",
    "        'percentile_25': float(np.percentile(v, 25)),\n",
    "        'percentile_10': float(np.percentile(v, 10)),\n",
    "        'min': float(np.min(v)),\n",
    "    }\n",
    "\n",
    "def _features_from_client_accs(accs_dict, meta, n_clients=None):\n",
    "\n",
    "    vals = np.array([accs_dict[k] for k in sorted(accs_dict.keys())], dtype=float)\n",
    "    base = _base_stats_from_accs(vals)\n",
    "\n",
    "    # Absolute stats (always compute them first)\n",
    "    row = {\n",
    "        'mean': base['mean'], 'median': base['median'], 'max': base['max'],\n",
    "        'percentile_90': base['percentile_90'], 'percentile_75': base['percentile_75'],\n",
    "        'percentile_25': base['percentile_25'], 'percentile_10': base['percentile_10'], 'min': base['min']\n",
    "    }\n",
    "    df_feat = pd.DataFrame([row])\n",
    "    if meta.get('featureset') in ('shape', 'shape_no_level'):\n",
    "        df_feat['std'] = df_feat[['min','percentile_10','percentile_25','median','percentile_75','percentile_90','max']].std(axis=1)\n",
    "        for c in ['median','max','percentile_90','percentile_75','percentile_25','percentile_10','min']:\n",
    "            df_feat[f'{c}_dm'] = df_feat[c] - df_feat['mean']\n",
    "\n",
    "    if meta.get('include_n_clients', False):\n",
    "        df_feat['n_clients'] = n_clients if n_clients is not None else np.nan\n",
    "\n",
    "    feat_cols = meta['selected_features']\n",
    "    missing = [c for c in feat_cols if c not in df_feat.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Feature mismatch. Missing columns for model: {missing}\")\n",
    "    return df_feat[feat_cols], float(base['mean'])\n",
    "\n",
    "def predict_global_from_loaded(accs_dict, n_clients=None):\n",
    "    X, mean_val = _features_from_client_accs(accs_dict, _META, n_clients)\n",
    "    y_model = _PIPE.predict(X)\n",
    "\n",
    "    if _META.get('target_mode', 'prob') == 'delta':\n",
    "        y = float(y_model[0] + mean_val)  \n",
    "    else:\n",
    "        y = float(_sigmoid(y_model[0]) if _META.get('logit_target', False) else y_model[0])\n",
    "\n",
    "\n",
    "    return float(np.clip(y, 0.0, 1.0))\n",
    "\n",
    "\n",
    "def seed_for(n_clients: int, max_iter: int, trial: int) -> int:\n",
    "    rnd = random.Random(base_seed + 1000*max_iter + 37*n_clients + 17*trial + 7919)\n",
    "    rc = rnd.randint(0, 10_000_000)\n",
    "    return base_seed + (trial - 1) + 1000*max_iter + 37*n_clients + 2*rc\n",
    "\n",
    "\n",
    "\n",
    "all_results = []\n",
    "best_vs_predicted = []\n",
    "\n",
    "for n_clients in n_clients_list:\n",
    "    print(f\"\\n> n_clients = {n_clients}\")\n",
    "\n",
    "    for max_iter in max_iters:\n",
    "        print(f\"  max_iter = {max_iter}\")\n",
    "        complete_static = Counter()\n",
    "        lottery_count = 0\n",
    "\n",
    "        for trial in range(1, n_trials + 1):\n",
    "            trial_seed = seed_for(n_clients, max_iter, trial)\n",
    "            random.seed(trial_seed)\n",
    "            np.random.seed(trial_seed)\n",
    "\n",
    "            X_train_pool, X_test, y_train_pool, y_test = train_test_split(\n",
    "                X_full, y_full, test_size=0.2, random_state=trial_seed, stratify=y_full\n",
    "            )\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            X_train_pool_scaled = scaler.fit_transform(X_train_pool)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "            partitions = split_data_equal(\n",
    "                X_train_pool_scaled, y_train_pool,\n",
    "                n_clients=n_clients,\n",
    "                shuffle=True,\n",
    "                random_seed=trial_seed\n",
    "            )\n",
    "\n",
    "            client_models, client_accs = train_models_fedlr(\n",
    "                partitions=partitions,\n",
    "                random_seed=trial_seed,\n",
    "                X_test=X_test_scaled,\n",
    "                y_test=y_test,\n",
    "                max_iter=max_iter\n",
    "            )\n",
    "\n",
    "            if isinstance(client_accs, dict):\n",
    "                client_accs_dict = {int(k): float(v) for k, v in client_accs.items()}\n",
    "                client_acc_list = [client_accs_dict[i] for i in sorted(client_accs_dict)]\n",
    "            else:\n",
    "                client_acc_list = [float(a) for a in client_accs]\n",
    "                client_accs_dict = {i: a for i, a in enumerate(client_acc_list)}\n",
    "\n",
    "\n",
    "            df_res = evaluate_coalitions2(\n",
    "                client_models=client_models,\n",
    "                client_global_accuracies=client_accs_dict,\n",
    "                n_clients=n_clients,\n",
    "                aggregator_func=aggregate_lr_models,\n",
    "                X_test=X_test_scaled,\n",
    "                y_test=y_test,\n",
    "                corrupt_client_indices=[],\n",
    "                approach=approach\n",
    "            )\n",
    "\n",
    "\n",
    "            df_ne = find_nash_equilibria_v2(df_res)\n",
    "            if not df_ne.empty:\n",
    "                for coalition in df_ne.index:\n",
    "                    complete_static[coalition] += 1\n",
    "\n",
    "            accs_simple = {int(cid): float(acc) for cid, acc in client_accs_dict.items()}\n",
    "            payoff_f = predict_global_from_loaded(accs_simple, n_clients=n_clients)\n",
    "\n",
    "            # Actual global accuracy for the grand coalition\n",
    "            full_coalition_str = \"1\" * n_clients\n",
    "            row_full = df_res.loc[df_res['Combination'] == full_coalition_str, 'Global Accuracy']\n",
    "            actual_global_acc = row_full.iloc[0] if not row_full.empty else np.nan\n",
    "\n",
    "            # Collect results\n",
    "            all_results.append({\n",
    "                'n_clients':        n_clients,\n",
    "                'max_iter':         max_iter,\n",
    "                'trial':            trial,\n",
    "                'predicted_global': payoff_f,\n",
    "                'actual_global':    actual_global_acc\n",
    "            })\n",
    "\n",
    "            # Best Client Accuracy vs Prediction\n",
    "            best_client_acc = float(np.nanmax(client_acc_list)) if len(client_acc_list) else np.nan\n",
    "            best_vs_predicted.append({\n",
    "                'n_clients': n_clients,\n",
    "                'trial': trial,\n",
    "                'best_client_acc': best_client_acc,\n",
    "                'predicted_global': payoff_f,\n",
    "                'actual_global': actual_global_acc\n",
    "            })\n",
    "\n",
    "            # Lottery check\n",
    "            has_incentive = any(acc > payoff_f for acc in client_acc_list)\n",
    "            if not has_incentive:\n",
    "                lottery_count += 1\n",
    "\n",
    "\n",
    "        complete_count = sum(complete_static.values())\n",
    "        counts_df = pd.DataFrame([{\n",
    "            'n_clients': n_clients,\n",
    "            'max_iter': max_iter,\n",
    "            'Complete_Occurrences': complete_count,\n",
    "            'Lottery_Occurrences': lottery_count\n",
    "        }])\n",
    "        fname = f\"Nash_Counts_{approach}_nclients_{n_clients}_maxiter_{max_iter}.csv\"\n",
    "        out_path = os.path.join(save_dir, fname)\n",
    "        counts_df.to_csv(out_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_path = os.path.join(save_dir, \"predicted_vs_actual_all_trials.csv\")\n",
    "results_df.to_csv(results_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "df_best_pred = pd.DataFrame(best_vs_predicted)\n",
    "best_pred_path = os.path.join(save_dir, \"best_client_vs_predicted_global.csv\")\n",
    "df_best_pred.to_csv(best_pred_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eef128",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(all_results)\n",
    "results_df = results_df.dropna(subset=['actual_global'])\n",
    "results_df['error'] = results_df['predicted_global'] - results_df['actual_global']\n",
    "optimistic_count = (results_df['error'] > 0).sum()\n",
    "pessimistic_count = (results_df['error'] < 0).sum()\n",
    "avg_optimism = results_df.loc[results_df['error'] > 0, 'error'].mean()\n",
    "avg_pessimism = results_df.loc[results_df['error'] < 0, 'error'].mean()\n",
    "\n",
    "# RMSE\n",
    "rmse = np.sqrt((results_df['error'] ** 2).mean())\n",
    "\n",
    "print(\"\\n=== Estimator Bias Summary ===\")\n",
    "print(f\"Total trials: {len(results_df)}\")\n",
    "print(f\"Optimistic trials: {optimistic_count} ({optimistic_count/len(results_df):.2%})\")\n",
    "print(f\"Pessimistic trials: {pessimistic_count} ({pessimistic_count/len(results_df):.2%})\")\n",
    "print(f\"Average optimism bias: {avg_optimism:.4f}\")\n",
    "print(f\"Average pessimism bias: {avg_pessimism:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "error_report_path = os.path.join(save_dir, \"estimator_error_report.csv\")\n",
    "results_df.to_csv(error_report_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1298501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pattern = os.path.join(save_dir, f\"Nash_Counts_{approach}_nclients_*_maxiter_{max_iters[0]}.csv\")\n",
    "files = sorted(glob.glob(pattern))\n",
    "\n",
    "\n",
    "counts_all = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
    "counts_all = counts_all.sort_values(\"n_clients\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(counts_all[\"n_clients\"], counts_all[\"Complete_Occurrences\"], marker=\"o\", label=\"Complete information\")\n",
    "plt.plot(counts_all[\"n_clients\"], counts_all[\"Lottery_Occurrences\"], marker=\"s\", label=\"Lottery occurrences\")\n",
    "plt.xlabel(\"Number of clients\")\n",
    "plt.ylabel(f\"Occurrences (out of {n_trials})\")\n",
    "plt.title(f\"Spmabase FedLR: Complete information vs Lottery rates across clients\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_dir, \"occurrences_vs_clients_counts.png\"), dpi=200)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "counts_all[\"Complete_Rate\"]  = counts_all[\"Complete_Occurrences\"]  / n_trials\n",
    "counts_all[\"Lottery_Rate\"] = counts_all[\"Lottery_Occurrences\"] / n_trials\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(counts_all[\"n_clients\"], counts_all[\"Complete_Rate\"], marker=\"o\", label=\"Complete rate\")\n",
    "plt.plot(counts_all[\"n_clients\"], counts_all[\"Lottery_Rate\"], marker=\"s\", label=\"Lottery rate\")\n",
    "plt.xlabel(\"Number of clients\")\n",
    "plt.ylabel(\"Rate\")\n",
    "plt.ylim(0, 1)\n",
    "plt.title(f\"Spmabase: Complete information vs Lottery rates across clients\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "counts_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00b85bd",
   "metadata": {},
   "source": [
    "### FedFor _ Spambase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9473e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/abbaszal/Documents/Thesis_Project_Spambase/data/spambase.data'  # adjust if needed\n",
    "df = pd.read_csv(file_path, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a6a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUNDLE_PATH = \".../linear_fedfor_delta_noN/linear_shape_no_level_none_delta_noN_leaveout_spambase.joblib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e75e191",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-8\n",
    "n_clients_list = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "n_trials = 100\n",
    "base_seed = 42\n",
    "max_depths = [100]     \n",
    "approach = 'fedfor'\n",
    "\n",
    "save_dir = \"/.../spambase_fedfor_linear\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7655c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = df.iloc[:, :-1].to_numpy()\n",
    "y_full = df.iloc[:, -1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5444ecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_bundle = joblib.load(BUNDLE_PATH)\n",
    "pipeline = _bundle[\"pipeline\"]\n",
    "_saved_features = list(_bundle[\"features\"])\n",
    "\n",
    "\n",
    "_META = {\n",
    "    \"selected_features\": _saved_features,\n",
    "    \"featureset\": \"shape_no_level\",\n",
    "    \"include_n_clients\": False,   \n",
    "    \"target_mode\": \"delta\",        \n",
    "    \"logit_target\": False,          \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def _base_stats_from_accs(accs_1d: np.ndarray) -> dict:\n",
    "    \n",
    "    v = np.asarray(accs_1d, dtype=float)\n",
    "    return {\n",
    "        'mean': float(np.mean(v)),\n",
    "        'median': float(np.median(v)),\n",
    "        'max': float(np.max(v)),\n",
    "        'percentile_90': float(np.percentile(v, 90)),\n",
    "        'percentile_75': float(np.percentile(v, 75)),\n",
    "        'percentile_25': float(np.percentile(v, 25)),\n",
    "        'percentile_10': float(np.percentile(v, 10)),\n",
    "        'min': float(np.min(v)),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def _features_from_client_accs(accs_dict: dict, meta: dict, n_clients: int | None = None) -> tuple[pd.DataFrame, float]:\n",
    "\n",
    "    vals = np.array([accs_dict[k] for k in sorted(accs_dict.keys())], dtype=float)\n",
    "    base = _base_stats_from_accs(vals)\n",
    "\n",
    "\n",
    "    row = {\n",
    "        'mean': base['mean'], 'median': base['median'], 'max': base['max'],\n",
    "        'percentile_90': base['percentile_90'], 'percentile_75': base['percentile_75'],\n",
    "        'percentile_25': base['percentile_25'], 'percentile_10': base['percentile_10'], 'min': base['min']\n",
    "    }\n",
    "    df_feat = pd.DataFrame([row])\n",
    "\n",
    "\n",
    "    if meta.get('featureset') in ('shape', 'shape_no_level'):\n",
    "        df_feat['std'] = df_feat[['min','percentile_10','percentile_25','median','percentile_75','percentile_90','max']].std(axis=1)\n",
    "        for c in ['median','max','percentile_90','percentile_75','percentile_25','percentile_10','min']:\n",
    "            df_feat[f'{c}_dm'] = df_feat[c] - df_feat['mean']\n",
    "\n",
    "    \n",
    "    if meta.get('include_n_clients', False):\n",
    "        df_feat['n_clients'] = n_clients if n_clients is not None else np.nan\n",
    "\n",
    "    feat_cols = meta['selected_features']\n",
    "\n",
    "    return df_feat[feat_cols], float(base['mean'])\n",
    "\n",
    "def predict_global_from_loaded(accs_dict: dict, n_clients: int | None = None) -> float:\n",
    "\n",
    "    X, mean_val = _features_from_client_accs(accs_dict, _META, n_clients)\n",
    "    y_model = float(pipeline.predict(X)[0])  \n",
    "    y = y_model + mean_val                  \n",
    "    return float(np.clip(y, 0.0, 1.0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_results = []\n",
    "best_vs_predicted = []\n",
    "\n",
    "\n",
    "\n",
    "for n_clients in n_clients_list:\n",
    "    print(f\"\\n> n_clients = {n_clients}\")\n",
    "\n",
    "    for max_depth in max_depths:\n",
    "        print(f\"  max_depth = {max_depth}\")\n",
    "        counts_complete = Counter()\n",
    "        lottery_count = 0\n",
    "\n",
    "        for trial in range(1, n_trials + 1):\n",
    "            trial_seed = base_seed + trial + 1000 * max_depth + 37 * n_clients\n",
    "            random.seed(trial_seed)\n",
    "            np.random.seed(trial_seed)\n",
    "\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_full, y_full, test_size=0.2, random_state=trial_seed  \n",
    "            )\n",
    "\n",
    "            partitions = split_data_equal(\n",
    "                X_train, y_train,\n",
    "                n_clients=n_clients,\n",
    "                shuffle=True,\n",
    "                random_seed=trial_seed\n",
    "            )\n",
    "\n",
    "            client_models, client_accs = train_models_fedfor(\n",
    "                partitions=partitions,\n",
    "                X_test=X_test,\n",
    "                y_test=y_test,\n",
    "                max_depth=max_depth,\n",
    "                trial_seed=trial_seed\n",
    "            )\n",
    "\n",
    "            df_res = evaluate_coalitions2(\n",
    "                client_models=client_models,\n",
    "                client_global_accuracies=client_accs,\n",
    "                n_clients=n_clients,\n",
    "                aggregator_func=FederatedForest,\n",
    "                X_test=X_test,\n",
    "                y_test=y_test,\n",
    "                corrupt_client_indices=[],\n",
    "                approach=approach\n",
    "            )\n",
    "\n",
    "\n",
    "            df_ne = find_nash_equilibria_v2(df_res)\n",
    "            if not df_ne.empty:\n",
    "                for coalition in df_ne.index:\n",
    "                    counts_complete[coalition] += 1\n",
    "\n",
    "            accs_simple = {int(cid): float(acc) for cid, acc in client_accs.items()}\n",
    "            payoff_f = predict_global_from_loaded(accs_simple, n_clients=n_clients)\n",
    "\n",
    "            # Actual Global Accuracy for grnd coalition\n",
    "            full_coalition_str = \"1\" * n_clients\n",
    "            row_full = df_res.loc[df_res['Combination'] == full_coalition_str, 'Global Accuracy']\n",
    "            actual_global_acc = float(row_full.iloc[0]) if not row_full.empty else np.nan\n",
    "\n",
    "    \n",
    "            all_results.append({\n",
    "                'n_clients': n_clients,\n",
    "                'max_depth': max_depth,\n",
    "                'trial': trial,\n",
    "                'predicted_global': payoff_f,\n",
    "                'actual_global': actual_global_acc\n",
    "            })\n",
    "\n",
    "            # Best client vs prediction\n",
    "            best_client_acc = float(np.max(list(accs_simple.values()))) if accs_simple else np.nan\n",
    "            best_vs_predicted.append({\n",
    "                'n_clients': n_clients,\n",
    "                'trial': trial,\n",
    "                'best_client_acc': best_client_acc,\n",
    "                'predicted_global': payoff_f,\n",
    "                'actual_global': actual_global_acc\n",
    "            })\n",
    "\n",
    "            # Incentive check\n",
    "            has_incentive = any(acc > payoff_f for acc in accs_simple.values())\n",
    "            if not has_incentive:\n",
    "                lottery_count += 1\n",
    "\n",
    "\n",
    "        complete_count = sum(counts_complete.values())\n",
    "        counts_df = pd.DataFrame([{\n",
    "            'n_clients': n_clients,\n",
    "            'max_iter': max_depth,\n",
    "            'Complete_Occurrences': complete_count,\n",
    "            'Lottery_Occurrences': lottery_count\n",
    "        }])\n",
    "        fname = f\"Nash_Counts_{approach}_nclients_{n_clients}_maxiter_{max_depth}.csv\"\n",
    "        out_path = os.path.join(save_dir, fname)\n",
    "        counts_df.to_csv(out_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_path = os.path.join(save_dir, \"predicted_vs_actual_all_trials.csv\")\n",
    "results_df.to_csv(results_path, index=False)\n",
    "\n",
    "\n",
    "df_best_pred = pd.DataFrame(best_vs_predicted)\n",
    "best_pred_path = os.path.join(save_dir, \"best_client_vs_predicted_global.csv\")\n",
    "df_best_pred.to_csv(best_pred_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bc569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(all_results)\n",
    "results_df = results_df.dropna(subset=['actual_global'])\n",
    "results_df['error'] = results_df['predicted_global'] - results_df['actual_global']\n",
    "optimistic_count = (results_df['error'] > 0).sum()\n",
    "pessimistic_count = (results_df['error'] < 0).sum()\n",
    "avg_optimism = results_df.loc[results_df['error'] > 0, 'error'].mean()\n",
    "avg_pessimism = results_df.loc[results_df['error'] < 0, 'error'].mean()\n",
    "\n",
    "# RMSE\n",
    "rmse = np.sqrt((results_df['error'] ** 2).mean())\n",
    "\n",
    "print(\"\\n=== Estimator Bias Summary ===\")\n",
    "print(f\"Total trials: {len(results_df)}\")\n",
    "print(f\"Optimistic trials: {optimistic_count} ({optimistic_count/len(results_df):.2%})\")\n",
    "print(f\"Pessimistic trials: {pessimistic_count} ({pessimistic_count/len(results_df):.2%})\")\n",
    "print(f\"Average optimism bias: {avg_optimism:.4f}\")\n",
    "print(f\"Average pessimism bias: {avg_pessimism:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "error_report_path = os.path.join(save_dir, \"estimator_error_report.csv\")\n",
    "results_df.to_csv(error_report_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c556cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pattern = os.path.join(save_dir, f\"Nash_Counts_{approach}_nclients_*_maxiter_{max_iters[0]}.csv\")\n",
    "files = sorted(glob.glob(pattern))\n",
    "\n",
    "\n",
    "counts_all = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n",
    "counts_all = counts_all.sort_values(\"n_clients\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(counts_all[\"n_clients\"], counts_all[\"Complete_Occurrences\"], marker=\"o\", label=\"Complete information\")\n",
    "plt.plot(counts_all[\"n_clients\"], counts_all[\"Lottery_Occurrences\"], marker=\"s\", label=\"Lottery occurrences\")\n",
    "plt.xlabel(\"Number of clients\")\n",
    "plt.ylabel(f\"Occurrences (out of {n_trials})\")\n",
    "plt.title(f\"Spmabase FedFor: Complete information vs Lottery rates across clients\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_dir, \"occurrences_vs_clients_counts.png\"), dpi=200)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "counts_all[\"Complete_Rate\"]  = counts_all[\"Complete_Occurrences\"]  / n_trials\n",
    "counts_all[\"Lottery_Rate\"] = counts_all[\"Lottery_Occurrences\"] / n_trials\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(counts_all[\"n_clients\"], counts_all[\"Complete_Rate\"], marker=\"o\", label=\"Complete rate\")\n",
    "plt.plot(counts_all[\"n_clients\"], counts_all[\"Lottery_Rate\"], marker=\"s\", label=\"Lottery rate\")\n",
    "plt.xlabel(\"Number of clients\")\n",
    "plt.ylabel(\"Rate\")\n",
    "plt.ylim(0, 1)\n",
    "plt.title(f\"Spmabase: Complete information vs Lottery rates across clients\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "counts_all\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
